{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5hw6ldIkpSPuIbO+BwJ26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samuel-K95/iCog-PC-Task/blob/main/PC_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step by step breakdown of Predictive Coding."
      ],
      "metadata": {
        "id": "TK9h1MglVmKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Input Clamping\n",
        "\n",
        "    - Fix the input layer to the data (e.g., an image or signal).\n",
        "    - This is the “sensory input” that higher layers must explain.\n",
        "\n",
        "2. Initialize Latent States\n",
        "\n",
        "    - Randomly initialize hidden variables\n",
        "    - These represent initial “guesses” about hidden causes of the input.\n",
        "\n",
        "3. Top-Down Prediction\n",
        "\n",
        "    - Each higher layer generates predictions for the layer below\n",
        "\n",
        "4. Prediction Error Computation\n",
        "\n",
        "    - For every layer, compute the mismatch:\n",
        "    - These errors measure surprise or “unexplained” input.\n",
        "\n",
        "5. Bottom-Up Error Propagation\n",
        "\n",
        "    - Errors flow upwards in the hierarchy.\n",
        "\n",
        "    - Lower-layer errors inform higher layers about how their predictions failed.\n",
        "\n",
        "6. Inference: Update Latent States\n",
        "\n",
        "    - Latent states are updated iteratively to reduce total error\n",
        "    - This fast loop continues until the system “settles” (converges).\n",
        "\n",
        "7. Recompute Predictions & Errors\n",
        "\n",
        "    - After each latent update, recompute predictions and errors.\n",
        "\n",
        "    - Repeat steps (3–6) for several iterations.\n",
        "\n",
        "    - This ensures the network stabilizes into a consistent explanation of the input.\n",
        "\n",
        "8. Weight Update (Learning)\n",
        "\n",
        "    - Once inference converges, update generative weights\n",
        "    - Local Hebbian-like rule: depends only on presynaptic activity and postsynaptic error.\n",
        "\n",
        "9. Repeat for New Inputs\n",
        "\n",
        "    - Restart the cycle with the next data sample (or minibatch).\n",
        "\n",
        "    - Over time, weights adapt so predictions improve, reducing average error."
      ],
      "metadata": {
        "id": "gKJ2JuZUbsdW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07gWe7gXUAQA"
      },
      "outputs": [],
      "source": []
    }
  ]
}